# 作业1: Bait 游戏

**张祎扬 （181840326  181840326@smail.nju.edu.cn ）**

（南京大学， 匡亚明学院， 南京， 210046）



**摘    要：**这是一个基于GVGAI游戏框架的编程项目，主要有四个任务，分别完成不同的搜索任务。

**关键词：**深度优先搜索、深度受限搜索、A*搜索、蒙特卡洛树搜索

## 1. 引言

在解决搜索问题时，我们常用几种搜索策略。其中，深度优先搜索是一种纵向搜索，它的空间复杂度低，时间复杂度高；而深度受限的深度优先搜索则在这一基础上对时间和空间的开销进行了平衡。A*算法看起来要更优秀一些，设计启发式函数是这其中的关键。蒙特卡洛树搜索则加入了概率，在大量搜索中平衡均值和不确定性。本次实验就是基于这几种树搜索方法的探索。

## 2. 实验内容

### 2.1 任务1

#### 2.1.1 任务内容：针对第一个关卡，实现深度优先搜索。

- 在游戏一开始就使用深度优先搜索找到成功的路径通关，记录下路径，并在之后每一步按照路径执行动作。
- 在搜索时避免回路，使用**StateObservation类的equalPosition**方法判断状态是否相等。
- 通过**CompetitionParameters.ACTION_TIME**来设置足够的时间来允许搜索。

#### 2.2.2 实验过程及代码实现

首先我观察了作业提供的样例，`smapleRandom.Agent`,发现它使用了比较重要的类`StateObservation`和`Observation`,于是我先观察了这两个文件中都有哪些方法。在这一过程中我参考了一个介绍GVG-AI框架的网站[1],这个网站介绍了一些常用的方法和提供的API接口，以及如何创建一个新的Agent。

![](/Users/zhangyiyang/Desktop/屏幕快照 2019-10-03 下午8.43.30.png)

从这个介绍可以看出，新的Agent必须包括以上这两种方法，同时Agent方法需要继承core.player.AbstractPlayer.java.然后我决定先大致按照`sampleRandom.Agent`搭好大致框架，再修改其中的act方法。

**关于代码中的一些变量创建，我有一些初步的想法。**为了避免走回路，肯定需要判断走完某一步之后的状态是否重新出现过，这就需要一个列表来存放所有已经搜索过的状态，还需要一个函数来判断状态是否出现过。另外，由于是深度优先搜索，直接搜索到成功的通关路径，再返回这个路径列表，所以需要一个列表来存放路径，相应地要有一个整数用来指示列表的元素的下标。

首先我在Agent类的开头对以上设想的几个变量进行了声明，然后在`public Agent`中对其进行了初始化。接着编写了一个函数来判断状态是否相等。判断的原理是遍历用来存放所有已经搜索过的状态的列表，使用StateObservation类的equalPosition方法判断状态是否相等，如果相等则返回true，如果遍历完列表后都没有返回true则返回flase。

关于搜索函数的主体部分，我的想法是首先要有一个布尔变量来判断是否找到了成功通关的路径，再使用递归的方法，当没有找到成功的路径时，则继续调用自身。伪代码的设想如下：

```java
boolean success = false;
private void depthfirst(StateObservation so){
  if(success)return;//if the successful path is found then return 
  //if the new so has not been searcched yet
  add the new so to the searched list;//judge if the new so has been searched 
  if win{
    //advanced judgement for the gameover state
    success = true;
    return;
  }
  //if not win then search again
  for(action in available actions){
    add new action to searched path;
    copy the state and advance action;
    if the advance state has not been searched yet//if the next state is new then search again else quit the search
      depthfirst();
    if(!success)//后进先出
      remove the latest action in the list
  }
}
```

在写主体代码时我基本参考了以上设想的框架完成了`depthfirst()`函数，然后在最终的`act()`中调用了这个函数。

**一点bug：**因为深度优先搜索的原则是后进先出，所以需要弹出列表的最后一个元素。本来我类比了python，以为index可以直接取-1，后来发生了一点报错。然后我参考了一些java教程，取列表最后一个元素的代码是`list.get(list.size()-1)`.运行之后第一关的游戏界面快速闪过，然后发生了报错，观察报错，发现是后面的任务的Agent没有找到。于是我把后面的任务先注释掉，发现Agent成功地通关了。最后的结果如下：

![](/Users/zhangyiyang/Desktop/屏幕快照 2019-10-04 下午1.48.26.png)

### 2.2 任务2

#### 2.2.1 任务内容 实现深度受限的深度优先搜索

- 修改为每一步进行一次深度搜索。
- 不需要搜索到通关，而是到一定深度，再设计一个启发式函数判断局面好坏。
- 设置时间较小，需要在设置时间内完成一次决策。

#### 2.2.2 实验过程及代码实现

在这个任务中，有一些地方和以上是相同的，虽然是深度受限的搜索，但是仍然是深度优先的，只不过需要加上一个深度的限制，就是对搜索次数的限制。同样地需要设计函数判断当前的状态是否被重复搜索过。

这其中，最大的不同，在于我需要构造一个启发式函数来判断局面好坏。根据书上[2]的解释，大多数的最佳优先搜索算法的$$f$$由**启发函数**(heuristic function)构成：
$$
h(n)=结点n到目标结点的最小代价路径的代价估计值
$$
根据任务的描述，已知的只有目标和钥匙的坐标还有当前agent的位置，所以我觉得评判标准只能是当前agent到目标的距离远近，并且越近越好。这就有点类似于**贪婪最佳优先搜索**，它试图扩展离目标最近的结点，因此它只用启发式信息，即$$f(n)=h(n)$$.由于在这个游戏中agent只能沿直角边前进，所以我打算将曼哈顿距离作为评判标准。假如agent已经取得了钥匙，那么启发式函数就计算它到目标的曼哈顿距离；假如agent还没有取得钥匙，那么启发式函数就计算它到钥匙和目标的曼哈顿距离之和。

这样分析下来，整个过程就被拆分成了明确的几部分。

- 深度优先搜索部分仍与任务一类似，只不过加入了深度的限制，所以在递归的时候要加入一个对当前的深度的判断。
- 任务一中使用到的判断当前状态是否出现过的函数仍然需要再次使用。
- 在主函数的步骤中需要加入对启发值的比较和判断。启发值更低的动作成为最优解。
- 启发式函数的值是基于坐标的曼哈顿距离，根据精灵是否已经得到钥匙而分为两种情况。

然后我简要分析一下主要的几个函数的伪代码。

```java
private void limitdepthfirst(StateObservation so,int limit)
{
  if best action is found
    return;
  add new so to the searched list;
  if gameover
    //further judgement for win or lose
    best action = action
    heuristic value=0
    best action is found;
  else if limit=0
    //compare the heuristic value and determine the best action
  else
    for action in available action
      get so copy and advance state
      if the so has not been searched yet
        if limit equals the LIMIT we set 
          initialize the best action and heuristic value;
        limitdepthfirst(copy,limit-1);
}
```

```java
private double countHeuristicValue(StateObservation so)
{
  a series of initialization;
  if the agent has key
    value = the distance between the agent and the goal;
  else
    value = the distance between the agent and the goal+the distance between the agent and the key;
  return value;
}
```

在真正开始实现之后我又发现了一个问题。深度优先搜索是直接搜索到解，然后返回所有的动作，再一次执行完毕。而深度受限的深度优先搜索不能保证总能搜索到通关，所以它并不能返回一系列完整的动作，它只能保证每一次选择的动作是基于启发式函数来说最优的动作，并且只返回一个动作，所以我不仅需要一个列表来记录所有搜索过的状态，还需要一个列表来记录真实返回过的动作产生的状态，同时在每次返回一个新动作的时候，清空用来记录搜索过的状态的列表。

最后在根据以上设想写代码时，我完全复制了任务一中的代码，并且在它的基础上进行修改。

**一点问题：**当要解决判断精灵是否拥有钥匙的问题时，我发现StateObservation类有这样一个方法`getAvatarType()`,它返回一个int值，但是我参考了doc文件夹中对该类的说明（一个html文件），发现这里面并没有对这个方法的介绍，我在该类的源码里也没有发现我想要的说明。

![](/Users/zhangyiyang/Desktop/屏幕快照 2019-10-05 下午7.09.14.png)

然后我参考了GVGAI官网[3]的介绍，发现还是没有这个方法……

![](/Users/zhangyiyang/Desktop/屏幕快照 2019-10-05 下午7.21.46.png)

后来经过我**不懈努力的尝试**，假如精灵还没拥有钥匙，那么这个方法的返回值就是1.

在大致完成代码时，运行却遇到了一个报错（其实IDEA在编辑代码的时候就有报错了）

![](/Users/zhangyiyang/Desktop/屏幕快照 2019-10-05 下午7.47.04.png)

这我就很不得其解了，**讲义给的代码为什么会报错！！！**结果原来是上面的`Arraylist`并没有用简括号声明类型……改完之后整个世界都美好了。

一开始我把LIMIT设置为10，我发现avatar在拿到钥匙之后，在两个箱子周围疯狂徘徊很久，最后回到了目标。然后我一路下调LIMIT，发现情况依然没有什么变化，虽然它像一个智障一样疯狂徘徊，但在经过一段时间后还是可以回到终点的。但是当LIMIT小于等于5以后它就开始疯狂报错了……

这里我有两个问题想解决，一个是它为什么报错，还有一个是它为什么在两个位置之间疯狂徘徊。**但是时间有限，我作业要来不及了，所以有缘再继续吧……**

### 2.3 任务3

#### 2.3.1 任务内容 A*算法

- 在任务2的基础上，将深度优先搜索换成A*算法
- 尝试在第二关、第三关中使用A*算法

#### 2.3.2 实验过程及代码实现

根据书[4]上的定义，A*搜索对结点的评估结合了$$g(n)$$,即到达此结点已经花费的代价，和$$h(n)$$,从该结点到目标结点所花代价：
$$
f(n) = g(n) + h(n)
$$
由于$$g(n)$$是从开始结点到结点$$n$$的路径代价,而$$h(n)$$是从结点n到目标结点到最小代价路径的估计值，因此
$$
f(n) = 经过结点n的最小代价解
$$
所以其实在主函数上，与之前的深度优先搜索差别不大，主要的改动在于启发函数的计算方式改变了。在这一任务中，对每个结点的信息记录更为复杂，所以我决定编写一个`MyNode`类，具体参考了任务4中的`SimpleTreeNode`类，这样可以方便地查找父结点、子结点，同时记录深度。对于启发式函数的设计，我的想法是，把结点的深度作为到达此结点已经花费的代价，而之前所用的曼哈顿距离是该结点到目标结点话费的最小代价。由此就得到了启发式函数。

在写`MyNode`类的时候，我实现了以下功能：`expand()`用来对某一个结点进行扩展，添加它的子结点。`find_mother()`用于找到某一个子结点的根结点，这在返回最后的动作的时候非常有用。

原先我还模仿蒙特卡洛树的Agent，实现了一个`AstarPlayer`类，后来由于debug耗时巨大，也找不到问题，所以我进行了简化，也把之前放在`MyNode`类的Astar搜索方法直接放进了act函数中。

由于搜索时间有限制，所以限制时间的代码和`SampleRandom`基本类似，当剩余时间大于一次搜索平均时间的2倍，并且大于最低限制的时候，不断进入循环。在进入循环之前先进行初始化，用当前的状态初始化一个结点，并把它作为根结点，即搜索的开始。然后还有一个列表`active_nodes`用于存放当前的活跃结点，即所有边缘的叶结点。在主循环中主要完成以下任务：遍历所有叶结点，找到总代价最小的最优叶结点，然后调用`expand()`方法扩展该结点，并将新扩展的子结点添加到叶结点列表中，再将之前找到的最优结点从列表中移除。这样始终保持列表中的是所有最边缘的叶结点。随着搜索的不断推进，在时间耗尽前，搜索的范围将不断扩大。

搜索结束后，遍历所有叶结点，找到代价最小的那个结点，用`find_mother()`方法找到它的根结点，从而可以返回一个相应的动作。

很遗憾，在这个任务上我耗费了大量的时间和精力，一遍一遍地确证我的代码的思路，一遍一遍的调试。我想也许是我本身对JAVA语法不够熟悉，因为毕竟是直接上手，很多概念虽然在使用但其实并不清楚。到目前为止，我还是没有通过调试让这个程序跑起来，也就无从验证我写的A*算法的正确性，所以我在以上对我的思路进行了阐释。

我想说的是，实验过程中我确实会遇到很多意想不到的困难。有的bug很低级，但是却让我寻找了很久，但也有的bug的出现真正是因为我的理解有偏差。在完成任务3的过程中，我经历了数次长达6至8小时的折磨，代码也被一改再改，不断注释，又不断删除注释。在这个过程中，除了意识到自己的菜以外，更多的是学会了保持平和的心态，和无论怎么样都保持耐心的品质。

即便如此，对接下来的每一个任务，我还是充满了信心。

### 2.4 任务4

**经过这周四新课的启发**，我知道了这个是蒙特卡洛树搜索。

首先在一开始定义了几个变量。`NUM_ACTIONS`表示子结点的数量，`ROLLOUT_DEPTH`是rollout环节完全随机搜索的深度。K是根号2，`actions`是一个动作数组。

**首先阅读SingleTreeNode.java**

SingleTreeNode中有一些元素，`state`是当前状态，`parent`是父结点，`children`是子结点数组，数组大小为`NUM_ACTIONS`,`m_rnd`是随机数，`m_depth`是当前深度。

`uct()`方法：返回一个树结点。首先把`bestValue`设为最小负数，遍历树结点的每一个子结点，设置它们的`childValue`和`uctValue`.其中`childValue`就相当于当前的均值$$Q(k)$$,它是用`totValue`除以访问次数，分母上的`epsilon`用来处理次数为0的情况。
$$
uctValue=Q(k)+\sqrt{\frac{2lnn}{n_{k}+epsilon}}
$$
从代码实现可以看出，$$n$$是总次数，$$n_{k}$$是子结点的访问次数，epsilon用来控制计算情况，所以`uctValue`的两项分别是均值和不确定性，当搜索的总次数越多，均值越有说服力，不确定性越低。该方法的最后比较了当前的`uctValue`和`bestValue`,并且更新`bestValue`和`selected`,返回`uctValue`最大的子结点。

`expand()`方法用于扩展一个子结点。

`notFullyExpanded()`方法用于判断所有子结点是否都扩展完全。

`treePolicy()`方法：当游戏没有结束，并且rollout深度没有达到最大限制的时候进入循环。在循环内，如果子结点没有被完全扩展，就继续扩展子结点，否则的话就调用`uct()`方法，返回`uctValue`最大的子结点。

`finishRollout()`方法：如果当前深度大于等于限制，则rollout结束。如果游戏结束，则rollout也结束。

`value()`方法：如果游戏结束，player输了，用当前得分减去`HUGE_NEGATIVE`;如果player赢了，用当前得分加上`HUGE_POSITIVE`作为`rawScore`.

`rollOut()`方法：如果rollout没有结束(用`finishRollout()`方法判断)，随机选择一个动作并更新当前状态。并令delta为当前状态的rawscore（不超过最大最小double数的限制），返回delta。

`backUp()`方法：从最后一个子结点开始往回回溯并更新`totValue`值和visit的次数。

`mostVisitedAction()`方法返回访问最多次的结点的索引。

`mctSearch()`方法的参数是`ElapsedCpuTimer elapsedTimer`,里面设置了每一次搜索的平均时间，花费的总时间，剩下的时间，和迭代次数。当剩余的时间大于两倍的搜索平均时间并且剩余时间大于限制的时候，进入搜索循环。在搜索循环中，调用`treePolicy()`方法选中一个子结点，再从这个子结点往下rollout，返回delta值，最后调用`backUp()`方法从最后一个结点往上更新这条路径上所有的结点的参数。这就完成了一次蒙特卡洛树搜索，即bandit和rollout的结合。

**接着阅读SingleMCTSPlayer.java**

首先设置了根结点`m_root`和随机数`m_rnd`,`init()`方法对其初始化。`run()`方法中首先对根结点调用`mctSearch()`方法，然后调用`mostVisited()`方法，并返回action值（索引）。

**最后阅读Agent.java**

在`Agent()`类中，对一系列动作数组和数组的长度进行了初始化。在act函数中，首先初始化了mctsPlayer，然后调用run方法返回一个最佳action的索引，最后返回动作列表中该索引的动作。

**总结：**蒙特卡洛树算法主要由两部分组成，第一部分是用bandit选择一次子结点（概率），第二部分是从这个选择的子结点随机rollout下去，并不断更新路径上每一个结点的信息。根据公式
$$
value = Q(k)+\sqrt{\frac{2lnn}{n_{k}}}
$$
可以求的均值和不确定性之和（这其中又加入了噪声来平衡数据），最终经过大量搜索之后，选择最佳的动作。

## 3.结束语

本文详细记录了我在完成任务一过程中的代码思路和心路历程，无论什么时候回头看，它都将是一笔宝贵的财富。

**致谢：**

感谢一些同学在我因作业进展不顺利时给予我的鼓励和安慰，让我一直坚持尽自己最大的努力完成。

**References:**

\[1\][][http://www.gvgai.net](http://www.gvgai.net/)

\[2\]《人工智能：一种现代的方法》3.5 有信息（启发式）的搜索策略

\[3\]http://gvgai.net/forwardModel.php

\[4\]《人工智能：一种现代的方法》3.5.2 A*搜索：缩小总评估代价